---
title: "Session_03"
author: "Archana Chittoor"
date: "December 06, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
### NLP Workshop Session 3 - Topic Modelling Hands-On

Load the tm library
```{r warning=FALSE,message=FALSE}
install.packages("tm", repos = "http://cran.us.r-project.org")
library(tm)
```

####Simple text analysis example
```{r warning=FALSE}
text <- "I like to eat broccoli and bananas.
I ate a banana and spinach smoothie for breakfast.
Chinchillas and kittens are cute.
My sister adopted a kitten yesterday.
Look at this cute hamster munching on a piece of broccoli."

# create a corpus
a_corpus <- SimpleCorpus(VectorSource(text))
```
#### Preprocessing of the text
```{r warning=FALSE}
# remove punctuation
a_corpus <- tm_map(a_corpus, removePunctuation)
writeLines(as.character(a_corpus[[1]]))

# Strip digits
a_corpus <- tm_map(a_corpus, removeNumbers)
writeLines(as.character(a_corpus[[1]]))

# remove stopwords
a_corpus <- tm_map(a_corpus, removeWords, stopwords("english"))
writeLines(as.character(a_corpus[[1]]))

# Strip extra whitespace from a text document. Multiple whitespace characters are collapsed to a single blank.
a_corpus<- tm_map(a_corpus, stripWhitespace)

# Good practice to check every now and then
writeLines(as.character(a_corpus[[1]]))
```
```{r warning=FALSE}
# create the document-term matrix
a_dtm <- DocumentTermMatrix(a_corpus)
inspect(a_dtm)
```
#### LDA
As per LDA, 
Every Document is a mixture of Topics
Every Topic is a mixture of Words

For example, we could imagine a two-topic model of American news, with one topic for “politics” and one for “entertainment.” The most common words in the politics topic might be “President”, “Congress”, and “government”, while the entertainment topic may be made up of words such as “movies”, “television”, and “actor”. 
  Importantly, words can be shared between topics; a word like “budget” might appear in both equally.

```{r warning=FALSE, message=FALSE}

install.packages("topicmodels", repos = "http://cran.us.r-project.org")
library(topicmodels)

# Run the LDA algorithm with k as 2 for generating a 2-topic model
lda <- LDA(a_dtm, k = 2)
# view the topics
lda.topics <- as.matrix((topics(lda,6)))
topics(lda)

# view the terms
lda.terms <-as.matrix(terms(lda))
terms(lda)
```
```{r warning=FALSE, message=FALSE}
# Topic modelling with news articles date set 

install.packages("dplyr", repos = "http://cran.us.r-project.org")
library(dplyr)

data("acq")
acq

# first document
acq[[1]]
```

#### Data pre-processing
```{r warning=FALSE, message=FALSE}
# remove punctuation
acq <- tm_map(acq, removePunctuation)
# Strip digits
acq <- tm_map(acq, removeNumbers)
# remove stopwords
acq <- tm_map(acq, removeWords, stopwords("english"))
acq <- tm_map(acq, removeWords, "said")
# remove whitespace
acq <- tm_map(acq, stripWhitespace)
# Good practice to check every now and then
writeLines(as.character(acq[[30]]))
```

```{r warning=FALSE, message=FALSE}

install.packages("tidytext", repos = "http://cran.us.r-project.org")
library(tidytext)

# convert to tidy format
acq_td <- tidy(acq)
acq_td

# separate into words
acq_tokens <- acq_td %>%
  select(-places) %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words, by = "word")

# most common words
acq_tokens %>%
  count(word, sort = TRUE)
```

```{r warning=FALSE, message=FALSE}

# create a DTM
acq_dtm <- DocumentTermMatrix(acq)

inspect(acq_dtm)

# Run LDA with k = 5
# set a seed so that the output of the model is predictable
acq_lda <- LDA(acq_dtm, k = 2, control = list(seed = 1234))
acq_lda

# list the topics
acq_lda.topics <- as.matrix((topics(acq_lda,6)))
topics(acq_lda)

# list the terms
acq_lda.terms <-as.matrix(terms(acq_lda))
terms(acq_lda)
```

The tidytext package provides the method tidy() for extracting the per-topic-per-word probabilities, called β (“beta”), from the model.
```{r warning=FALSE, message=FALSE}

acq_topics <- tidy(acq_lda, matrix = "beta")
acq_topics
```
This has turned the model into a one-topic-per-term-per-row format. For each combination, the model computes the probability of that term being generated from that topic. 
```{r warning=FALSE, message=FALSE}
# Plot the top terms in the dataset

install.packages("ggplot2", repos = "http://cran.us.r-project.org")
library(ggplot2)

acq_top_terms <- acq_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

acq_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered()
```
LDA also models each document as a mixture of topics. We can examine the per-document-per-topic probabilities, called “gamma”, with the matrix = "gamma" argument to tidy().

```{r warning=FALSE, message=FALSE}

# Computing the gamma values  
acq_documents <- tidy(acq_lda, matrix = "gamma")
  acq_documents
```

#### Topic modelling example wih Jane Austen books  
```{r warning=FALSE, message=FALSE}

install.packages("janeaustenr", repos = "http://cran.us.r-project.org")
library("janeaustenr")
# load the data from six of Jane Austen's books
  austen <- austen_books()
  
# separate text into word tokens
  words <- austen %>%
    unnest_tokens(word, text)
  
# Remove the stop words and list the words according to their counts
  word_counts <- words %>%
    anti_join(stop_words) %>%
    count(book, word, sort = TRUE) %>%
    ungroup()
  
# List the top 10 words
  top_n(word_counts,10)
  
```
```{r warning=FALSE, message=FALSE}

#create a document-term matrix with the text
  austen_dtm <- word_counts %>%
    cast_dtm(book, word, n)
  
  austen_dtm
  
# run the LDA algorithm on the Austen DTM
  austen_lda <- LDA(austen_dtm, k = 5, control = list(seed = 1234))
  
  
  austen_topics <- tidy(austen_lda, matrix = "beta")
  
# list the most common topics
  top_n(austen_topics, 10)
  
#list the top terms
  top_terms <- austen_topics %>%
    group_by(topic) %>%
    top_n(5, beta) %>%
    ungroup() %>%
    arrange(topic, -beta)
```

```{r warning=FALSE, message=FALSE, echo=FALSE}

# plot the top terms 
  top_terms %>%
    mutate(term = reorder(term, beta)) %>%
    ggplot(aes(term, beta, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free") +
    coord_flip()
  
# Compute the gamma values
  austen_gamma <- tidy(austen_lda, matrix = "gamma")
  
  top_n(austen_gamma, 10)
```
