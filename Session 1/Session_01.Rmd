---
title: "Introduction to Natural Language Processing (NLP) - Session 1 Hands-On"
author: "Archana Chittoor"
date: "November 15, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Session 2 Hands-On
In this session, we will start with the basic data types in R. We will focus mainly on String data types and functions to manipulate them, which are essential for Text Analysis. We will then proceed to our class example and perform some tokenization and lemmatization.

First, we install and load the required packages

```
install.packages("tidyverse")
install.packages("stringr")

library(tidyverse)
library(stringr)
```
Let us look at some of the basic data types in R

1. Numbers

Assign values to variables using the '<-' operator. Optionally, we can use '=' operator

```
var <- 5
var = 5

variable <- 23
variable <- 23.6567
```
To view the value, run the variable name or use the print() function
```
var
print(var)
```

We can perform all sorts of basic operations and save the numbers into variables
For example,

```
x <- 6
y <- sqrt(x)

y
```
Defining lists (also called Vectors). We use the c command to create lists

```
l <- c(4,5,6,7)
l[3]
l[6]
```
We can perform operations on these lists
```
l*6
mean(l)
```
Listing all variables defined in a session
```
ls()
```
Using numeric() function to create lists
```
m <- numeric(5)
m
```
To determine the type of a variable, use typeof() command
The type of a number is "double"
```
typeof(l)
typeof(var)
```
2. Strings

We specify a string by using single or double quotes:

```
str <- "Welcome"
str

string <- "Hello"
```

The type of a string is "character"
```
typeof(str)
str_list <- c("Welcome", "to", "NLP","Workshop")
str_list
str_list[4]
```

3. Factors

Factors are objects which are used to categorize data and store them as levels. 
```
dir <- c("East","West","East","North","North","East","West","East","South")
```
Apply the factor function.
```
factor_dir <- factor(dir)

print(factor_dir)
print(is.factor(factor_dir))
str(factor_dir)
```
4. Data Frames

Data Frames can contain many vectors of different types.
For example, a data frame may contain several lists, and each list might be a list of factors, strings, or numbers.
```
a <- c(1,2,3,4)
b <- c(2,4,6,8)
levels <- factor(c("A","B","A","B"))
df <- data.frame(first=a,
                      second=b,
                      rank=levels)
df
```
5. Dates

The lubridate package makes dealing with dates much easier.
It comes with conversion, extraction and other functionalities that help in processing dates in your data.
```
install.packages("lubridate") 
library(lubridate)

today()
today()+1

today() + dyears(1)

leap_year(2016)

?lubridate
?today
```

### More on Strings:
```
string1 <- "This is a string"
string2 <- 'If you want to include a "quote" inside a string, use single quotes'  
```
To include a literal single or double quote in a string you can use \ to “escape” it
  
double_quote <- "\"" # or '"'
single_quote <- '\"' # or "'"  

The printed representation of a string is not the same as string itself, 
because the printed representation shows the escapes. To see the raw contents of the string, use writeLines():
```  
x <- c("\"", "\\")
x
#> [1] "\"" "\\"
writeLines(x)
writeLines(double_quote)
```
Special characters

```
x <- "\u00b5"
x
```
```
?"'"
```
Gives a list of all special characters
View all the String finctions by typing "str_"

1. To find the length of a string

```
str_length(string1)

str_length(str_list)
```

2. Combining strings
To concatenate two or more strings, use str_c()

```
str_c(string1, string2)
str_c(string1, string2, sep = ", ")
```
Adding prefix and suffix
```
str_c("prefix-", c("a", "b", "c"), "-suffix")
```

Collapsing strings
```
str_c(c("a", "b", "c"), collapse = ", ")
```

We can also use paste to combine strings
```
string <- paste(c('a', 'b', 'cd'), collapse='')
string

string <- paste(c('a', 'b', 'cd'), collapse='|')
string

string <- paste0('a', 'b', 'cd') # equivalent to collapse=''
string

string <- paste0('a', 1:10)
string

```

3. String Subsets
You can extract parts of a string using str_sub(). 
We also specify the start and end positions (inclusive) which need to be extracted from the string

```
fruits <- c("apple", "pear", "banana")
str_sub(fruits, 1, 3)
```

To count backwards, give negative numbers 
```
str_sub(fruits, -3, -1)
```

4. Changing Case

```
str_to_lower(str_list)
str_to_upper(str_list)
str_to_title(str_list)
```

5. Sorting

```
str_sort(fruits)
fruits
str_order(fruits)
```

6. Pattern Matching - Used to find patterns in strings
```
str_view(fruits, "ar")
```

It can be used with regular expressions
```
fruits
```

Detecting matches
```
str_detect(fruits, "a")
```

Counting matches
```
str_count(fruits, "a")

str_count(fruits, "[aeiou]")
```

Extract matches
```
length(sentences)
head(sentences)

colors <- c("red", "orange", "yellow", "green", "blue", "purple")
color_match <- str_c(colors, collapse = "|")
color_match

has_color <- str_subset(sentences, color_match)
matches <- str_extract(has_color, color_match)
head(matches)

more <- sentences[str_count(sentences, color_match) > 1]
str_view_all(more, color_match)

str_extract(more, color_match)

str_extract_all(more, color_match)
```

Replacing Matches

Replaces the first occurrence

```
str_replace(fruits, "[aeiou]", "_")
```

Replaces all occurrences

```
str_replace_all(fruits, "[aeiou]", "_")

fruit_list <- c("1 apple", "2 pears", "3 bananas")
str_replace_all(fruit_list, c("1" = "one", "2" = "two", "3" = "three"))
```

7. Splitting Strings

```
sentences %>%
  head(10) %>% 
  str_split(" ")

# To return a matrix
sentences %>%
  head(5) %>% 
  str_split(" ", simplify = TRUE)

sentences %>%
  head(5) %>% 
  str_split(" ", n = 4, simplify = TRUE)
```

8. GREP and GSUB
```
str <- "Fall 2019: NLP Workshop" 
```

Grep to find strings
```
grepl("NLP", str)

grepl("AI", str)
```

Gsub to replace 
```
gsub("Fall 2019", "Spring 2020", str)
```
Let's look at our class example for Text Analysis
Install and load the required packages
```
install.packages("tidyverse")
install.packages("tokenizers")

library(tidyverse)
library(tokenizers)
```

### Tokenization

We will learn to read the text file into R, to break it into words and sentences, and to turn it into n-grams.

#### Reading a text file
There are a number of ways to read a text file into R. The simplest way to read a plain text file is with the readLines() function, which reads each line as a separate character vector.

```
London <- readLines("C:/Users/archa/Documents/London.txt")
print(London)
```

We can see that there are three lines in the file, each contained in a character vector. 
We can combine all of these character vectors into a single character vector using the paste() function, adding a space between each of them.

```
London <- paste(London, collapse = " ")
print(London)
```

The tokenizer package also includes the function tokenize_sentences that splits a text into sentences 
```
sentences <- tokenize_sentences(London)
sentences
```

The output is given as a character vector, a one-dimensional R object consisting only of elements represented as characters.
Notice that the output pushed each sentence into a separate element.

We will use the tokenize_words function from the tokenizers package to split the text into individual words.
```
words <- tokenize_words(London)
words

length(words)
```

The reason that the length is equal to 1 is that the function tokenize_words returns a list object with one entry per document in the input. Our input only has a single document and therefore the list only has one element. 
To see the words inside the first document, we use the symbol [[1]] to select just the first element of the list

```
length(words[[1]])
```

It is possible to pair the output of the sentence tokenizer with the word tokenizer. 
```
sentence_words <- tokenize_words(sentences[[1]])
sentence_words
```

We can see that there are totally 3 sentences
```
length(sentence_words)
```

To see how many words there are in each sentence
```
length(sentence_words[[1]])
length(sentence_words[[2]])
```

This can become quite cumbersome. There is an easier way. The sapply() function applies its second argument to every element of its first argument. 
As a result, we can calculate the length of every sentence in the paragraph with one line of code

```
sapply(sentence_words, length)
```

We can see that there are three sentences of length 14, 24 and 10
Load the packages and Data

### Stemming Versus Lemmatizing

```
install.packages("pacman")
library(pacman)
pacman::p_load(textstem, dplyr)

data(presidential_debates_2012)
```

#### Example 1
```
drive_words <- c('driver', 'drive', 'drove', 'driven', 'drives', 'driving')
stem_words(drive_words)

lemmatize_words(drive_words)
```

#### Example 2
```
drive_words <- c('driver', 'drive', 'drove', 'driven', 'drives', 'driving')
stem_words(drive_words)

lemmatize_words(drive_words)

be_words <- c('are', 'am', 'being', 'been', 'be')

stem_words(be_words)

lemmatize_words(be_words)
```

For our class example
```
London <- readLines("C:/Users/archa/Documents/London.txt")

London <- paste(London, collapse = " ")
print(London)
```

Stemming
```
stem_strings(London)
```

Lemmatizing
```
lemmatize_strings(London)
```

#### Hunspell Lemma Dictionary
This lemmatization uses the hunspell package to generate lemmas.
```
lemma_dictionary_hs <- make_lemma_dictionary(London, engine = 'hunspell')
lemmatize_strings(London, dictionary = lemma_dictionary_hs)
```
#### koRpus Lemma Dictionary
This lemmatization uses the koRpus package and the TreeTagger program to generate lemmas. You’ll have to get TreeTagger set up, preferably in your machine’s root directory.

```
lemma_dictionary_tt <- make_lemma_dictionary(London, engine = 'treetagger')
lemmatize_strings(London, lemma_dictionary_tt)
```
Next week, we will perform some more Text Mining and introduce Word Clouds and Sentiment Analysis
